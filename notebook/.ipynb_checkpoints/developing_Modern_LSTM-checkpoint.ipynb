{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430a0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a98746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25038a9",
   "metadata": {},
   "source": [
    "## Always check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0135db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f974c3f",
   "metadata": {},
   "source": [
    "# 2 ways to do this:\n",
    "### a. make a linear layer which will itself initialise weights and multiply them with the input tensor\n",
    "### b. make parameters and manually write the code for matrix multiplication\n",
    " \n",
    "## Although 1st one is more efficient and error free, going on with the 2nd one for learning purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd51a30",
   "metadata": {},
   "source": [
    "## *Considering input (x) of batch size = 1 for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2197a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Cell\n",
    "\n",
    "class LSTM_Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self,nin):\n",
    "        super(LSTM_Cell,self).__init__()\n",
    "        self.input_size=nin\n",
    "        self.tanh=nn.Tanh()\n",
    "         \n",
    "        #input gate weight\n",
    "        self.in_xt=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.in_xb=nn.Parameter(torch.Tensor(1,1))\n",
    "        self.in_ht=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.in_Sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        #cell memory gate weight\n",
    "        self.cm_xt=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.cm_xb=nn.Parameter(torch.Tensor(1,1))\n",
    "        self.cm_ht=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.cm_Tanh=nn.Tanh()\n",
    "        \n",
    "        #forget gate weights\n",
    "        self.f_xt=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.f_xb=nn.Parameter(torch.Tensor(1,1))\n",
    "        self.f_ht=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.f_Sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        #output gate weights\n",
    "        self.op_xt=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.op_xb=nn.Parameter(torch.Tensor(1,1))\n",
    "        self.op_ht=nn.Parameter(torch.Tensor(1,self.input_size))\n",
    "        self.op_Sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.init_params()\n",
    "        \n",
    "        \n",
    "    def init_params(self):\n",
    "        for each_p in self.parameters():\n",
    "            nn.init.uniform_(each_p, -1,1)\n",
    "    \n",
    "    def input_gate(self,x,h):\n",
    "        inp=x @ self.in_xt.T + self.in_xb\n",
    "        hidden=h @ self.in_ht.T\n",
    "        return self.in_Sigmoid(inp+hidden)\n",
    "    \n",
    "    def cell_memory_gate(self,x,h):\n",
    "        inp=x @ self.cm_xt.T + self.cm_xb\n",
    "        hidden=h @ self.cm_ht.T\n",
    "        return self.cm_Tanh(inp+hidden)\n",
    "    \n",
    "    def forget_gate(self,x,h):\n",
    "        inp=x @ self.f_xt.T + self.f_xb\n",
    "        hidden=h @ self.f_ht.T\n",
    "        return self.f_Sigmoid(inp+hidden)\n",
    "    \n",
    "    def output_gate(self,x,h):\n",
    "        inp=x @ self.op_xt.T + self.op_xb\n",
    "        hidden=h @ self.op_ht.T\n",
    "        return self.op_Sigmoid(inp+hidden)\n",
    "    \n",
    "    def forward(self,x,tuple_of_c_h):\n",
    "        h_state_prev, c_state_prev = tuple_of_c_h\n",
    "        \n",
    "        #working through input gate\n",
    "        inp=self.input_gate(x,h_state_prev)\n",
    "        \n",
    "        #working through the forget gate\n",
    "        forget=self.forget_gate(x,h_state_prev)\n",
    "        \n",
    "        #working through the cell memory gate\n",
    "        c_m=self.cell_memory_gate(x,h_state_prev)    \n",
    "        \n",
    "        #working through the ouput gate\n",
    "        initial_op=self.output_gate(x,h)\n",
    "        \n",
    "        c_state_curr=forget*c_state_prev + inp*c_m\n",
    "        \n",
    "        h_state_curr=self.tanh(c_state_curr)*initial_op\n",
    "        \n",
    "        return (h_state_curr,c_state_curr)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f9e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Layer\n",
    "\n",
    "class LSTM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self,nin,hidden):\n",
    "        super(LSTM_Layer,self).init()\n",
    "        self.input_size=nin\n",
    "        self.hidden_size=hidden\n",
    "        \n",
    "        self.LSTM_cells=[LSTM_Cell(self.input_size) for each in range(self.hidden_size)]\n",
    "    \n",
    "    def forward(self,x,tuple_of_c_h):\n",
    "      \n",
    "        for i in range(self.hidden_size):\n",
    "            new_h_state=[]\n",
    "            new_c_state=[]\n",
    "            \n",
    "            h_state, c_state=tuple_of_c_h\n",
    "            \n",
    "            cell=self.LSTM_cells[i]\n",
    "            a,b=cell(x,(h_state[i],c_state[i]))\n",
    "            new_h_state.append(a)\n",
    "            new_c_state.append(b)\n",
    "            \n",
    "            return (new_h_state,new_c_state)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7227e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Net\n",
    "\n",
    "class LSTM_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nin,hidden,layers):\n",
    "        super(LSTM_Net,self).init()\n",
    "        self.input_size=nin\n",
    "        self.hidden_size=hidden\n",
    "        self.layers=layers\n",
    "        \n",
    "        self.LSTM_Layers=[LSTM_Layer(self.input_size,self.hidden_size) for each in range(self.layers)]\n",
    "        \n",
    "        #final layer\n",
    "        self.final=nn.Linear(self.hidden_size,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(x):\n",
    "        #batch size of x is 1\n",
    "        len_of_sentence,_=x.size()\n",
    "        \n",
    "        h_states=[torch.rand(self.hidden_size,self.input_size) for _ in range(self.layers)]\n",
    "        c_states=[torch.rand(self.hidden_size,self.input_size) for _ in range(self.layers)]\n",
    "        \n",
    "        #working through time steps\n",
    "        #time steps are same as the length of sentence, as you need to remember the entire sentence\n",
    "        \n",
    "        for time_step in range(len_of_sentence):\n",
    "            #x at that time step\n",
    "            x_t=x[time_step,:]\n",
    "            \n",
    "            for i in range(self.layers):\n",
    "                \n",
    "                layer=self.LSTM_Layers[i]\n",
    "                \n",
    "                h_states[i],c_states[i]=layer(x_t, (h_states[i],c_states[i]))\n",
    "                x_t=h_states[i]\n",
    "                                              \n",
    "            #passing final hidden state through a linear layer\n",
    "            out=self.final(h_states[-1])\n",
    "            return out                   \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afe0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
