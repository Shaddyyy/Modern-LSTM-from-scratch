{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "430a0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp Modern_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e35e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "689ec18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346b1fd",
   "metadata": {},
   "source": [
    "### Always check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f14db1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f974c3f",
   "metadata": {},
   "source": [
    "# 2 ways to do this:\n",
    "### a. make a linear layer which will itself initialise weights and multiply them with the input tensor\n",
    "### b. make parameters and manually write the code for matrix multiplication\n",
    " \n",
    "## Although 1st one is more efficient and error free, going on with the 2nd one for learning purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd51a30",
   "metadata": {},
   "source": [
    "## *Considering input (i.e. x) of batch size = 1 for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cd65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15215f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "115813fc",
   "metadata": {},
   "source": [
    "# READ ATTENTIVELY FOR THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953ffb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When you create an nn.Module on the CPU, the .parameters() function points to the params on the CPU ONLY.\n",
    "#So, if you create the params and move it to the GPU, the self.parameters() function will return an empty sequence\n",
    "\n",
    "#1.Its better to create the nn.Module entirely (including the params) on the cpu and moving its instance to the GPU\n",
    "#ORRRRR\n",
    "#1.Move the params to the GPU during the forward pass of the cell.\n",
    "\n",
    "#We'll go with the 1st to complicate things less and learn more, however in time I will update the forward pass to\n",
    "#be carried on the GPU only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2197a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "#LSTM Cell\n",
    "class LSTM_Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self,nin,hidden):\n",
    "        super(LSTM_Cell,self).__init__()\n",
    "        self.input_size=nin\n",
    "        self.hidden_size=hidden\n",
    "        self.tanh=nn.Tanh()\n",
    "         \n",
    "        #input gate weight\n",
    "        self.in_xt=nn.Parameter(torch.Tensor(self.hidden_size,self.input_size))\n",
    "        self.in_xb=nn.Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.in_ht=nn.Parameter(torch.Tensor(self.hidden_size,self.hidden_size))\n",
    "        self.in_Sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        #cell memory gate weight\n",
    "        self.cm_xt=nn.Parameter(torch.Tensor(self.hidden_size,self.input_size))\n",
    "        self.cm_xb=nn.Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.cm_ht=nn.Parameter(torch.Tensor(self.hidden_size,self.hidden_size))\n",
    "        self.cm_Tanh=nn.Tanh()\n",
    "        \n",
    "        #forget gate weights\n",
    "        self.f_xt=nn.Parameter(torch.Tensor(self.hidden_size,self.input_size))\n",
    "        self.f_xb=nn.Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.f_ht=nn.Parameter(torch.Tensor(self.hidden_size,self.hidden_size))\n",
    "        self.f_Sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        #output gate weights\n",
    "        self.op_xt=nn.Parameter(torch.Tensor(self.hidden_size,self.input_size))\n",
    "        self.op_xb=nn.Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.op_ht=nn.Parameter(torch.Tensor(self.hidden_size,self.hidden_size))\n",
    "        self.op_Sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.init_params()\n",
    "        \n",
    "        \n",
    "    def init_params(self):\n",
    "        for each_p in self.parameters():\n",
    "            nn.init.uniform_(each_p, -1,1)\n",
    "    \n",
    "    def input_gate(self,x,h):\n",
    "        inp=x @ self.in_xt.T + self.in_xb\n",
    "        hidden=h @ self.in_ht.T\n",
    "        return self.in_Sigmoid(inp+hidden)\n",
    "    \n",
    "    def cell_memory_gate(self,x,h):\n",
    "        inp=x @ self.cm_xt.T + self.cm_xb\n",
    "        hidden=h @ self.cm_ht.T\n",
    "        return self.cm_Tanh(inp+hidden)\n",
    "    \n",
    "    def forget_gate(self,x,h):\n",
    "        inp=x @ self.f_xt.T + self.f_xb\n",
    "        hidden=h @ self.f_ht.T\n",
    "        return self.f_Sigmoid(inp+hidden)\n",
    "    \n",
    "    def output_gate(self,x,h):\n",
    "        inp=x @ self.op_xt.T + self.op_xb\n",
    "        hidden=h @ self.op_ht.T\n",
    "        return self.op_Sigmoid(inp+hidden)\n",
    "    \n",
    "    def forward(self,x,tuple_of_c_h):\n",
    "        h_state_prev, c_state_prev = tuple_of_c_h\n",
    "        \n",
    "        #working through input gate\n",
    "        inp=self.input_gate(x,h_state_prev)\n",
    "        \n",
    "        #working through the forget gate\n",
    "        forget=self.forget_gate(x,h_state_prev)\n",
    "        \n",
    "        #working through the cell memory gate\n",
    "        c_m=self.cell_memory_gate(x,h_state_prev)    \n",
    "        \n",
    "        #working through the ouput gate\n",
    "        initial_op=self.output_gate(x,h)\n",
    "        \n",
    "        c_state_curr=forget*c_state_prev + inp*c_m\n",
    "        \n",
    "        h_state_curr=self.tanh(c_state_curr)*initial_op\n",
    "        \n",
    "        return (h_state_curr,c_state_curr)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f9e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "#LSTM Layer\n",
    "class LSTM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self,nin,hidden):\n",
    "        super(LSTM_Layer,self).__init__()\n",
    "        self.input_size=nin\n",
    "        self.hidden_size=hidden\n",
    "        \n",
    "        self.LSTM_cells=[LSTM_Cell(self.input_size,self.hidden_size) for _ in range(self.hidden_size)]\n",
    "    \n",
    "    def forward(self,x,tuple_of_c_h):\n",
    "        \n",
    "        h_state, c_state=tuple_of_c_h\n",
    "        new_h_state=[]\n",
    "        new_c_state=[]\n",
    "        \n",
    "        for i in range(self.hidden_size):\n",
    "            \n",
    "            \n",
    "            cell=self.LSTM_cells[i]\n",
    "            a,b=cell(x,(h_state[i],c_state[i]))\n",
    "            new_h_state.append(a)\n",
    "            new_c_state.append(b)\n",
    "            \n",
    "            return (new_h_state,new_c_state)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7227e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "#LSTM Net\n",
    "class LSTM_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nin,hidden,layers):\n",
    "        super(LSTM_Net,self).__init__()\n",
    "        self.input_size=nin\n",
    "        self.hidden_size=hidden\n",
    "        self.layers=layers\n",
    "        \n",
    "        self.LSTM_Layers=[LSTM_Layer(self.input_size,self.hidden_size) if each==0 else LSTM_Layer(self.hidden_size,self.hidden_size) for each in range(self.layers)]\n",
    "        \n",
    "        #pre_final layer\n",
    "        self.pre_final=nn.Linear(self.hidden_size,1)\n",
    "        #final_layer\n",
    "        self.final=nn.Linear(self.hidden_size,1,bias=False)\n",
    "        \n",
    "    def forward(x):\n",
    "        #batch size of x is 1\n",
    "        len_of_sentence,_=x.size()\n",
    "        \n",
    "        #initial hidden and cell states which will further get updated\n",
    "        h_states=[torch.rand(self.hidden_size,self.hidden_size).to(device) for _ in range(self.layers)]\n",
    "        c_states=[torch.rand(self.hidden_size,self.hidden_size).to(device) for _ in range(self.layers)]\n",
    "        \n",
    "        \n",
    "        #working through time steps\n",
    "        #time steps are same as the length of sentence, as you need to remember the entire sentence\n",
    "        for time_step in range(len_of_sentence):\n",
    "            #x at that time step\n",
    "            x_t=x[time_step]\n",
    "            \n",
    "            for i in range(self.layers):\n",
    "                \n",
    "                layer=self.LSTM_Layers[i]\n",
    "                \n",
    "                h_states[i],c_states[i]=layer(x_t, (h_states[i],c_states[i]))\n",
    "                x_t=h_states[i]\n",
    "                                              \n",
    "            #passing final hidden states through a linear layers\n",
    "            out=[]\n",
    "            for i in range(len(h_states[-1])):\n",
    "                temp_var=self.pre_final(h_states[-1][i])\n",
    "                out.append(temp_var)\n",
    "            \n",
    "            out=torch.Tensor(out)\n",
    "            out=self.final(out)\n",
    "            return out                   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "026838bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting notebook to vscode\n",
    "\n",
    "from nbdev.export import nb_export\n",
    "nb_export('developing_Modern_LSTM.ipynb','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e6211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
